import os
import sys
import subprocess
import torch
import whisperx
import pandas as pd
import math
from chatGpt_Improvement import generate_title_summary_and_speakers, correct_transcription_and_summary


# (×œ× ×—×•×‘×” â€“ ×œ×©× ×”×“×¤×¡×” ××¡×•×“×¨×ª)
from tabulate import tabulate

# × ×™×™×‘× ××ª Pipeline ×Ö¾pyannote.audio ×œ×©×™××•×© ×‘×–×™×”×•×™ ×“×•×‘×¨×™× (×œ×¨××ª ×”××©×¤×˜)
from pyannote.audio import Pipeline

# --------------------- ×”×’×“×¨×•×ª ×’×œ×•×‘×œ×™×•×ª ---------------------
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# ×˜×•×§×Ÿ ×œ-Hugging Face × ×˜×¢×Ÿ ×××©×ª× ×” ×”×¡×‘×™×‘×”
HUGGING_FACE_TOKEN = os.getenv("HUGGING_FACE_TOKEN")
HUGGING_FACE_TOKEN = "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"

# ×”×’×“×¨×ª ×¡×£ ×‘×¡×™×¡×™ ×œ×–×™×”×•×™ ××™×œ×™× ×‘×¢×™×™×ª×™×•×ª (×œ×©×™××•×© ×¢×–×¨ ×‘×¨××ª ×”××™×œ×”)
THRESHOLD_SCORE = 0.05

def format_time(seconds):
    """×¤×•×¨××˜ ×©× ×™×•×ª ×œ-MM:SS ×¢×‘×•×¨ ×”×¦×’×” ×™×“×™×“×•×ª×™×ª"""
    if seconds is None:
        return "N/A"
    m = int(seconds // 60)
    s = int(seconds % 60)
    return f"{m:02}:{s:02}"

def assign_speakers_to_words(word_segments, speaker_segments):
    """
    ××©×™×™×š ×œ×›×œ ××™×œ×” ×“×•×‘×¨ ×¢×œ ×¡××š ×–××Ÿ ×”×”×ª×—×œ×” ×©×œ×”.
    ×¢×‘×•×¨ ×›×œ ××™×œ×”, ×‘×•×“×§×™× ×× ×–××Ÿ ×”×”×ª×—×œ×” × ×•×¤×œ ×‘×ª×•×š ×¡×’×× ×˜ ×©×œ ×“×•×‘×¨.
    """
    for word in word_segments:
        word_start = word.get("start", 0)
        word["speaker"] = "×œ× ×™×“×•×¢"
        for seg in speaker_segments:
            if seg["start"] <= word_start <= seg["end"]:
                word["speaker"] = seg["speaker"]
                break
    return word_segments

def run_diarization(abs_path):
    """
    ×˜×•×¢×Ÿ ×•××¨×™×¥ ××ª ××•×“×œ ×–×™×”×•×™ ×”×“×•×‘×¨×™× ×‘×××¦×¢×•×ª Pipeline.from_pretrained
    ×¢×‘×•×¨ ×§×•×‘×¥ ×©××¢ × ×ª×•×Ÿ.
    ××—×–×™×¨×” ×¨×©×™××ª ×¡×’×× ×˜×™× ×¢× ×–×× ×™ ×”×ª×—×œ×”, ×¡×™×•× ×•×”×“×•×‘×¨.
    """
    speaker_segments = []
    try:
        print("ğŸ”¹ ×˜×•×¢×Ÿ ××•×“×œ ×–×™×”×•×™ ×“×•×‘×¨×™× (pyannote/speaker-diarization)...")
        diarization_pipeline = Pipeline.from_pretrained(
            "pyannote/speaker-diarization",
            use_auth_token=HUGGING_FACE_TOKEN
        )
        print("âœ… ××•×“×œ ×–×™×”×•×™ ×“×•×‘×¨×™× × ×˜×¢×Ÿ ×‘×”×¦×œ×—×”!")
    except Exception as e:
        print(f"âŒ ×˜×¢×™× ×ª ××•×“×œ ×–×™×”×•×™ ×“×•×‘×¨×™× × ×›×©×œ×”: {e}")
        print("âš ï¸ ×××©×™×š ×œ×œ× ×–×™×”×•×™ ×“×•×‘×¨×™×...")
        return speaker_segments

    try:
        print("ğŸ”¹ ××‘×¦×¢ ×–×™×”×•×™ ×“×•×‘×¨×™×...")
        diarization_result = diarization_pipeline({"uri": abs_path, "audio": abs_path})
        print("âœ… ×–×™×”×•×™ ×“×•×‘×¨×™× ×”×¦×œ×™×—! ×œ×”×œ×Ÿ ×”×¤×œ×˜:")
        for turn, _, speaker in diarization_result.itertracks(yield_label=True):
            print(f"{turn.start:.2f}s - {turn.end:.2f}s | ×“×•×‘×¨: {speaker}")
            speaker_segments.append({
                "start": turn.start,
                "end": turn.end,
                "speaker": speaker
            })
    except Exception as e:
        print("âŒ ×©×’×™××” ×‘×¢×ª ×‘×™×¦×•×¢ ×–×™×”×•×™ ×“×•×‘×¨×™×. ×××©×™×š ×œ×œ× ×–×™×”×•×™ ×“×•×‘×¨×™×.")
        print(e)

    return speaker_segments

def process_audio_file(file_path):
    """
    ×¤×•× ×§×¦×™×” ××¨×›×–×™×ª ×œ×¢×™×‘×•×“ ×§×•×‘×¥ ×©××¢:
      1. ×‘×•×“×§×ª FFmpeg, ×˜×•×§×Ÿ ×•×§×™×•× ×§×•×‘×¥ ×”××•×“×™×•.
      2. ×˜×•×¢× ×ª ××ª ××•×“×œ WhisperX, ××‘×¦×¢×ª ×ª××œ×•×œ ×•-Force Alignment.
      3. ××¨×™×¦×” ×–×™×”×•×™ ×“×•×‘×¨×™×.
      4. ××•×¡×¤×ª ××ª ×”××™×œ×™× ××”×¤×œ×˜, ××©×™×™×›×ª ×“×•×‘×¨×™× ×•××¡×× ×ª ××™×œ×™× ×‘×¢×™×™×ª×™×•×ª ×‘×¨××ª ×”××™×œ×”.
      5. ×‘×•× ×” DataFrame ×‘×¨××ª ×”××™×œ×”.
    """
    # ×‘×“×™×§×ª FFmpeg
    try:
        subprocess.run(["ffmpeg", "-version"], check=True, capture_output=True)
        print("âœ… FFmpeg ×–××™×Ÿ!")
    except FileNotFoundError:
        print("âŒ FFmpeg ×œ× × ××¦× ×‘××¢×¨×›×ª ××• ××™× ×• ××•×’×“×¨ ×‘-PATH.")
        sys.exit(1)

    # ×‘×“×™×§×ª ×˜×•×§×Ÿ
    if not HUGGING_FACE_TOKEN:
        print("âŒ HUGGING_FACE_TOKEN ×œ× ××•×’×“×¨ ×‘×¡×‘×™×‘×ª ×”×¢×‘×•×“×”.")
        sys.exit(1)

    # ×‘×“×™×§×” ×©×§×•×‘×¥ ×”××•×“×™×• ×§×™×™×
    abs_path = os.path.abspath(file_path)
    if not os.path.exists(abs_path):
        print(f"âŒ ×”×§×•×‘×¥ {abs_path} ×œ× ×§×™×™×!")
        sys.exit(1)
    print(f"ğŸ”¹ ××¢×‘×“ ×§×•×‘×¥ ××•×“×™×•: {abs_path}")

    # ×˜×¢×™× ×ª ××•×“×œ WhisperX ×•×ª×”×œ×™×š ×”×ª××œ×•×œ ×•×”-Alignment
    print("ğŸ”¹ ×˜×•×¢×Ÿ ××•×“×œ WhisperX...")
    whisper_model = whisperx.load_model("large-v2", device=DEVICE, compute_type="float32")
    alignment_model, metadata = whisperx.load_align_model(language_code="he", device=DEVICE)
    audio = whisperx.load_audio(abs_path)

    print("ğŸ”¹ ××‘×¦×¢ ×ª××œ×•×œ ×¨××©×•× ×™...")
    whisper_result = whisper_model.transcribe(audio)
    print("ğŸ”¹ ××¡×™×™× ×ª××œ×•×œ. ××–×”×” ×©×¤×”:", whisper_result.get("language", "×œ× ×™×“×•×¢"))

    print("ğŸ”¹ ××‘×¦×¢ Alignment...")
    aligned_result = whisperx.align(
        whisper_result["segments"],
        alignment_model,
        metadata,
        audio,
        DEVICE
    )
    print("âœ… Alignment ×”×•×©×œ×!")

    # ×”×¨×¦×ª ×–×™×”×•×™ ×“×•×‘×¨×™×
    speaker_segments = run_diarization(abs_path)

    # ××™×¡×•×£ ×›×œ ×”××™×œ×™× ××”×¤×œ×˜ (×¢×‘×•×¨ ×›×œ segment × ××¡×£ ×”×©×“×” "words")
    aligned_words = []
    if "segments" in aligned_result:
        for segment in aligned_result["segments"]:
            if "words" in segment:
                aligned_words.extend(segment["words"])
    else:
        print("âŒ ×œ× × ××¦××• segments ×‘×ª×•×¦××•×ª Alignment.")

    # ×©×™×•×š ×“×•×‘×¨×™× ×œ××™×œ×™× (×× ×§×™×™××™×)
    if speaker_segments:
        aligned_words = assign_speakers_to_words(aligned_words, speaker_segments)

    # ×‘× ×™×™×ª ×¨×©×™××ª × ×ª×•× ×™× ×œ×›×œ ××™×œ×”
    data = []
    for w in aligned_words:
        word_text = w.get("word", "").strip()
        start_time = w.get("start")
        end_time = w.get("end")
        score = w.get("score")
        speaker = w.get("speaker", "×œ× ×™×“×•×¢")

        if not word_text or start_time is None or end_time is None:
            continue

        try:
            predict_value = score if isinstance(score, (int, float)) else "N/A"
            score_percent = round(score * 100, 2) if isinstance(score, (int, float)) else "N/A"
        except:
            predict_value = "N/A"
            score_percent = "N/A"

        data.append({
            "××™×œ×”": word_text,
            "Predict": predict_value,
            "score": score,  # × ×©××•×¨ ××ª ×”×¢×¨×š ×”××§×•×¨×™
            "××—×•×– × ×™×‘×•×™": score_percent,
            "×”×ª×—×œ×”": format_time(start_time),
            "×¡×™×•×": format_time(end_time),
            "××©×š": round(end_time - start_time, 2),
            "×“×•×‘×¨": speaker
        })

    df_words = pd.DataFrame(data)
    df_words.sort_values(by=["×”×ª×—×œ×”"], inplace=True)

    # ×—×™×©×•×‘ ×¡×£ ×“×™× ××™ ×¢×‘×•×¨ ×›×œ ×”××™×œ×™× (global dynamic threshold)
    df_words["Predict_numeric"] = pd.to_numeric(df_words["Predict"], errors="coerce")
    mean_score = df_words["Predict_numeric"].mean()
    std_score = df_words["Predict_numeric"].std()
    dynamic_threshold = mean_score - std_score
    print(f"×××•×¦×¢ score: {mean_score:.4f}, ×¡×˜×™×™×ª ×ª×§×Ÿ: {std_score:.4f}, ×¡×£ ×“×™× ××™: {dynamic_threshold:.4f}")

    # ×¢×“×›×•×Ÿ ×¢××•×“×ª "×‘×¢×™×™×ª×™×ª" ×¢×‘×•×¨ ×”××™×œ×™× (global)
    df_words["×‘×¢×™×™×ª×™×ª"] = df_words["Predict_numeric"].apply(lambda x: True if pd.notnull(x) and x < dynamic_threshold else False)

    print("âœ… ×¢×™×‘×•×“ ×‘×¨××ª ×”××™×œ×” ×”×¡×ª×™×™× ×‘×”×¦×œ×—×”!")
    return df_words, aligned_result

def merge_word_and_segment_data(aligned_result):
    """
    ×××–×’ × ×ª×•× ×™ Alignment ×›×š ×©×›×œ ×©×•×¨×” ×‘-DataFrame ××™×™×¦×’×ª ××©×¤×˜/×¤×¡×§×”.
    ×× ××™×Ÿ ×ª×•×¦××•×ª, ××—×–×™×¨ DataFrame ×¢× ×¢××•×“×•×ª ×¨×™×§×•×ª ×›×“×™ ×œ×× ×•×¢ ×§×¨×™×¡×”.
    """
    sentences = []

    if "segments" in aligned_result and aligned_result["segments"]:
        for seg in aligned_result["segments"]:
            text = seg.get("text", "").strip()
            start = seg.get("start")
            end = seg.get("end")
            if start is None or end is None or not text:
                continue
            duration = end - start

            words = seg.get("words", [])
            if words:
                scores = [w.get("score") for w in words if isinstance(w.get("score"), (int, float))]
                avg_score = sum(scores) / len(scores) if scores else None
                std_seg = math.sqrt(sum((s - avg_score) ** 2 for s in scores) / len(scores)) if scores else 0
                seg_threshold = avg_score - std_seg if avg_score is not None else None

                speakers = [w.get("speaker", "×œ× ×™×“×•×¢") for w in words]
                majority_speaker = max(set(speakers), key=speakers.count) if speakers else "×œ× ×™×“×•×¢"

                problematic_words = [w.get("word", "").strip() for w in words
                                     if isinstance(w.get("score"), (int, float)) and seg_threshold is not None and w.get("score") < seg_threshold]
            else:
                avg_score = None
                majority_speaker = "×œ× ×™×“×•×¢"
                problematic_words = []

            sentences.append({
                "××©×¤×˜": text,
                "×”×ª×—×œ×”": format_time(start),
                "×¡×™×•×": format_time(end),
                "××©×š": round(duration, 2),
                "×××•×¦×¢ ××—×•×– × ×™×‘×•×™": round(avg_score * 100, 2) if avg_score is not None else "N/A",
                "×“×•×‘×¨": majority_speaker,
                "××™×œ×™× ×‘×¢×™×™×ª×™×•×ª": problematic_words
            })
    else:
        print("âŒ ×œ× × ××¦××• segments ×‘×ª×•×¦××•×ª Alignment. ××—×–×™×¨ DataFrame ×¨×™×§.")
        return pd.DataFrame(columns=["××©×¤×˜", "×”×ª×—×œ×”", "×¡×™×•×", "××©×š", "×××•×¦×¢ ××—×•×– × ×™×‘×•×™", "×“×•×‘×¨", "××™×œ×™× ×‘×¢×™×™×ª×™×•×ª"])

    df_sentences = pd.DataFrame(sentences)
    return df_sentences


if __name__ == "__main__":
    audio_file = "audio_files/call_water_fixed_first60s.wav"
    print(f"ğŸ”¹ ××¢×‘×“ ××ª ×”×§×•×‘×¥: {audio_file}")

    df_words, aligned_result = process_audio_file(audio_file)
    print("\nğŸ” ×‘×“×™×§×ª × ×ª×•× ×™× ×¨××©×•× ×™× ×¢×‘×•×¨ ××™×œ×™× (head):")
    print(df_words.head(20).to_string(index=False))

    df_sentences = merge_word_and_segment_data(aligned_result)
    print("\nğŸ” ×‘×“×™×§×ª × ×ª×•× ×™× ×¨××©×•× ×™× ×¢×‘×•×¨ ××©×¤×˜×™× (head):")
    print(df_sentences.head(20).to_string(index=False))

    out_csv_words = "output/full_transcription_words.csv"
    df_words.to_csv(out_csv_words, index=False, encoding="utf-8-sig")
    print(f"âœ… ×©××™×¨×” ×œ-CSV ×¢×‘×•×¨ ××™×œ×™× ×‘×•×¦×¢×”: '{out_csv_words}'")

    out_csv_sentences = "output/full_transcription_sentences.csv"
    df_sentences.to_csv(out_csv_sentences, index=False, encoding="utf-8-sig")
    print(f"âœ… ×©××™×¨×” ×œ-CSV ×¢×‘×•×¨ ××©×¤×˜×™× ×‘×•×¦×¢×”: '{out_csv_sentences}'")


### CHAT GPT IMPROVMENT
    print (" ****************          ### CHAT GPT IMPROVMENT")
   # ×§×¨×™××” ×©×œ ×”×§×•×‘×¥ "full_transcription_sentences.csv" ×•×”×›× ×ª DataFrame
    df = df_sentences
    transcript_df = df[['××©×¤×˜', '×“×•×‘×¨']].copy()

    # ×”×¤×§×ª ×›×•×ª×¨×ª, ×ª×§×¦×™×¨ ×•×¨×©×™××ª ×“×•×‘×¨×™× ×™×—×“ ×¢× ×ª××œ×•×œ ××¢×•×“×›×Ÿ
    title, summary, speakers, updated_transcript_df = generate_title_summary_and_speakers(transcript_df, audio_file)
    print("\n--- ×ª×•×¦××•×ª ×”×¤×§×ª ×›×•×ª×¨×ª, ×ª×§×¦×™×¨ ×•×¨×©×™××ª ×“×•×‘×¨×™× ---")
    print("×›×•×ª×¨×ª:", title)
    print("×¡×™×›×•×:", summary)
    print("×¨×©×™××ª ×“×•×‘×¨×™×:", speakers)
   # print("\n--- ×ª××œ×•×œ ×”×©×™×—×” ×”××¢×•×“×›×Ÿ ---")
   # print(updated_transcript_df.head(20).to_string(index=False))

    # ×ª×™×§×•×Ÿ ×”×ª××œ×•×œ ×‘×—×œ×•×§×” ×œ×‘××¦'×™× (×¢×“ 10 ××©×¤×˜×™× ×‘×›×œ ×¤×¢×)
    print("# ×ª×™×§×•×Ÿ ×”×ª××œ×•×œ ×‘×—×œ×•×§×” ×œ×‘××¦'×™× (×¢×“ 10 ××©×¤×˜×™× ×‘×›×œ ×¤×¢×)************************    ")
    result = correct_transcription_and_summary(updated_transcript_df, title, summary, speakers, audio_file)
    print("\n--- ×ª×•×¦××•×ª ×ª×™×§×•×Ÿ ×”×ª××œ×•×œ ---")
    print(result)

    print("# ×”×“×¤×¡×ª ×”×¨×©×•××•×ª ×”××¡×•× × ×•×ª ×©×©×•× ×• ----------------------------")
    # ×¡×™× ×•×Ÿ ×”×¨×©×•××•×ª ×‘×”×Ÿ ChangeAvailable ×”×•× True
    filtered_records = [record for record in result['transcript'] if record.get("ChangeAvailable") is True]
    for record in filtered_records:
        print(record)
